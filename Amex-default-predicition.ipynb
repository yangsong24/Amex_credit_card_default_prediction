{"cells":[{"cell_type":"code","execution_count":1,"id":"f7b367c6","metadata":{},"outputs":[{"data":{"text/plain":["'3.1.3'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import pyspark.sql.functions as F\n","\n","pyspark.__version__"]},{"cell_type":"code","execution_count":2,"id":"56a737da","metadata":{},"outputs":[],"source":["from pyspark import StorageLevel\n","from pyspark.sql import SparkSession, types\n","from pyspark.sql import functions as F\n","from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import LogisticRegression\n","import pandas as pd \n","import numpy as np "]},{"cell_type":"code","execution_count":3,"id":"816540d4","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder\\\n","    .appName(\"amex-app\")\\\n","    .master(\"local[*]\")\\\n","    .getOrCreate()"]},{"cell_type":"code","execution_count":4,"id":"56dc4541","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["train_df = spark.read.option('header','true').csv('gs://amex-data/amex-csv-data/train_data.csv').limit(20)\n","label_df = spark.read.option('header','true').csv('gs://amex-data/amex-csv-data/train_labels.csv')"]},{"cell_type":"markdown","id":"6a675737","metadata":{},"source":["**Defining Schema:**"]},{"cell_type":"code","execution_count":5,"id":"f7a6eb54","metadata":{},"outputs":[],"source":["types_map = {\n","    \"object\": types.StringType(),\n","    \"float64\": types.FloatType(),\n","    \"int64\": types.IntegerType(),\n","}\n","\n","# Known dtypes\n","string_dtypes = [\"customer_ID\", 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","date_dtypes = ['S_2']\n","# integer_dtypes = ['target']"]},{"cell_type":"code","execution_count":6,"id":"571f281b","metadata":{},"outputs":[],"source":["def create_spark_schema(series):\n","    fields = []\n","    \n","    for value in series:\n","        if value in string_dtypes:\n","            field = types.StructField(value, types.StringType(), True)\n","            \n","#         elif value in date_dtypes:\n","#             field = types.StructField(value, types.DateType(), True)\n","        \n","        else:\n","            field = types.StructField(value, types.FloatType(), True)\n","            \n","        fields.append(field)\n","    return types.StructType(fields)"]},{"cell_type":"code","execution_count":7,"id":"013d7a14","metadata":{},"outputs":[],"source":["train_schema = create_spark_schema(train_df.columns) \n","label_schema = create_spark_schema(label_df.columns)  "]},{"cell_type":"markdown","id":"48a87408","metadata":{},"source":["**Read data back with the defined schema**"]},{"cell_type":"code","execution_count":8,"id":"f15cc2be","metadata":{},"outputs":[],"source":["train_df = spark.read.option(\"header\", \"true\").csv(\"gs://amex-data/amex-csv-data/train_data.csv\", schema=train_schema)\n","label_df = spark.read.option(\"header\", \"true\").csv(\"gs://amex-data/amex-csv-data/train_labels.csv\", schema=label_schema)"]},{"cell_type":"code","execution_count":9,"id":"49e73979","metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[customer_ID: string, S_2: float, P_2: float, D_39: float, B_1: float, B_2: float, R_1: float, S_3: float, D_41: float, B_3: float, D_42: float, D_43: float, D_44: float, B_4: float, D_45: float, B_5: float, R_2: float, D_46: float, D_47: float, D_48: float, D_49: float, B_6: float, B_7: float, B_8: float, D_50: float, D_51: float, B_9: float, R_3: float, D_52: float, P_3: float, B_10: float, D_53: float, S_5: float, B_11: float, S_6: float, D_54: float, R_4: float, S_7: float, B_12: float, S_8: float, D_55: float, D_56: float, B_13: float, R_5: float, D_58: float, S_9: float, B_14: float, D_59: float, D_60: float, D_61: float, B_15: float, S_11: float, D_62: float, D_63: string, D_64: string, D_65: float, B_16: float, B_17: float, B_18: float, B_19: float, D_66: string, B_20: float, D_68: string, S_12: float, R_6: float, S_13: float, B_21: float, D_69: float, B_22: float, D_70: float, D_71: float, D_72: float, S_15: float, B_23: float, D_73: float, P_4: float, D_74: float, D_75: float, D_76: float, B_24: float, R_7: float, D_77: float, B_25: float, B_26: float, D_78: float, D_79: float, R_8: float, R_9: float, S_16: float, D_80: float, R_10: float, R_11: float, B_27: float, D_81: float, D_82: float, S_17: float, R_12: float, B_28: float, R_13: float, D_83: float, R_14: float, R_15: float, D_84: float, R_16: float, B_29: float, B_30: string, S_18: float, D_86: float, D_87: float, R_17: float, R_18: float, D_88: float, B_31: float, S_19: float, R_19: float, B_32: float, S_20: float, R_20: float, R_21: float, B_33: float, D_89: float, R_22: float, R_23: float, D_91: float, D_92: float, D_93: float, D_94: float, R_24: float, R_25: float, D_96: float, S_22: float, S_23: float, S_24: float, S_25: float, S_26: float, D_102: float, D_103: float, D_104: float, D_105: float, D_106: float, D_107: float, B_36: float, B_37: float, R_26: float, R_27: float, B_38: string, D_108: float, D_109: float, D_110: float, D_111: float, B_39: float, D_112: float, B_40: float, S_27: float, D_113: float, D_114: string, D_115: float, D_116: string, D_117: string, D_118: float, D_119: float, D_120: string, D_121: float, D_122: float, D_123: float, D_124: float, D_125: float, D_126: string, D_127: float, D_128: float, D_129: float, B_41: float, B_42: float, D_130: float, D_131: float, D_132: float, D_133: float, R_28: float, D_134: float, D_135: float, D_136: float, D_137: float, D_138: float, D_139: float, D_140: float, D_141: float, D_142: float, D_143: float, D_144: float, D_145: float]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"code","execution_count":10,"id":"3c5f3c73","metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[customer_ID: string, target: float]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["label_df"]},{"cell_type":"markdown","id":"12678817","metadata":{},"source":["**Data Preprocessing**"]},{"cell_type":"code","execution_count":11,"id":"37dffd35","metadata":{},"outputs":[],"source":["def add_suffix(names, suffix):\n","    return [name + suffix for name in names]"]},{"cell_type":"code","execution_count":12,"id":"1e211c74","metadata":{},"outputs":[],"source":["# # Known Columns\n","# info_cols = ['customer_ID', 'S_2']\n","# target_cols = ['target']\n","# cat_cols = [\n","#     'B_30', 'B_38', \n","#     'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","\n","\n","# # Define Numeric Columns\n","# excluded = info_cols + cat_cols\n","# num_cols = [col for col in train_df.columns if col not in excluded]\n","\n","# # Define Feature Columns\n","# features_cols =  cat_cols + num_cols\n","\n","# print(f\"Number of categoric cols: {len(cat_cols)}\")\n","# print(f\"Number of numeric cols: {len(num_cols)}\")"]},{"cell_type":"code","execution_count":13,"id":"8c8fe1f2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of categoric cols: 10\n"]}],"source":["# Known Columns\n","info_cols = ['customer_ID', 'S_2']\n","target_cols = ['target']\n","cat_cols = [\n","    'B_30', 'B_38', \n","    'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_68']\n","\n","\n","# Define Numeric Columns\n","excluded = info_cols + cat_cols\n","num_cols = [col for col in train_df.columns if col not in excluded]\n","\n","# Define Feature Columns\n","features_cols =  cat_cols + num_cols\n","\n","print(f\"Number of categoric cols: {len(cat_cols)}\")\n","# print(f\"Number of numeric cols: {len(num_cols)}\")"]},{"cell_type":"markdown","id":"57ce4d65","metadata":{},"source":["**Check null values**"]},{"cell_type":"code","execution_count":14,"id":"5c003805","metadata":{},"outputs":[],"source":["# Dict_Null = {col:train_df.filter(train_df[col].isNull()).count() for col in train_df.columns}\n","# Dict_Null"]},{"cell_type":"code","execution_count":15,"id":"09f08440","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/14 01:37:42 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["-RECORD 0--------------\n"," customer_ID | 0       \n"," S_2         | 5531451 \n"," P_2         | 45985   \n"," D_39        | 0       \n"," B_1         | 0       \n"," B_2         | 2016    \n"," R_1         | 0       \n"," S_3         | 1020544 \n"," D_41        | 2016    \n"," B_3         | 2016    \n"," D_42        | 4740137 \n"," D_43        | 1658396 \n"," D_44        | 274319  \n"," B_4         | 0       \n"," D_45        | 2017    \n"," B_5         | 0       \n"," R_2         | 0       \n"," D_46        | 1211699 \n"," D_47        | 0       \n"," D_48        | 718725  \n"," D_49        | 4985917 \n"," B_6         | 233     \n"," B_7         | 0       \n"," B_8         | 22268   \n"," D_50        | 3142402 \n"," D_51        | 0       \n"," B_9         | 0       \n"," R_3         | 0       \n"," D_52        | 29563   \n"," P_3         | 301492  \n"," B_10        | 0       \n"," D_53        | 4084585 \n"," S_5         | 0       \n"," B_11        | 0       \n"," S_6         | 0       \n"," D_54        | 2016    \n"," R_4         | 0       \n"," S_7         | 1020544 \n"," B_12        | 0       \n"," S_8         | 0       \n"," D_55        | 184803  \n"," D_56        | 2990943 \n"," B_13        | 49519   \n"," R_5         | 0       \n"," D_58        | 0       \n"," S_9         | 2933643 \n"," B_14        | 0       \n"," D_59        | 106725  \n"," D_60        | 0       \n"," D_61        | 598052  \n"," B_15        | 6923    \n"," S_11        | 0       \n"," D_62        | 758161  \n"," D_63        | 0       \n"," D_64        | 217442  \n"," D_65        | 0       \n"," B_16        | 2016    \n"," B_17        | 3137598 \n"," B_18        | 0       \n"," B_19        | 2016    \n"," D_66        | 4908097 \n"," B_20        | 2016    \n"," D_68        | 216503  \n"," S_12        | 0       \n"," R_6         | 0       \n"," S_13        | 0       \n"," B_21        | 0       \n"," D_69        | 194473  \n"," B_22        | 2016    \n"," D_70        | 94917   \n"," D_71        | 0       \n"," D_72        | 23708   \n"," S_15        | 0       \n"," B_23        | 0       \n"," D_73        | 5475595 \n"," P_4         | 0       \n"," D_74        | 21773   \n"," D_75        | 0       \n"," D_76        | 4908954 \n"," B_24        | 0       \n"," R_7         | 1       \n"," D_77        | 2513912 \n"," B_25        | 6923    \n"," B_26        | 2016    \n"," D_78        | 274319  \n"," D_79        | 75939   \n"," R_8         | 0       \n"," R_9         | 5218918 \n"," S_16        | 0       \n"," D_80        | 21773   \n"," R_10        | 0       \n"," R_11        | 0       \n"," B_27        | 2016    \n"," D_81        | 25687   \n"," D_82        | 4058614 \n"," S_17        | 0       \n"," R_12        | 56      \n"," B_28        | 0       \n"," R_13        | 0       \n"," D_83        | 194473  \n"," R_14        | 1       \n"," R_15        | 0       \n"," D_84        | 29563   \n"," R_16        | 0       \n"," B_29        | 5150035 \n"," B_30        | 2016    \n"," S_18        | 0       \n"," D_86        | 0       \n"," D_87        | 5527586 \n"," R_17        | 0       \n"," R_18        | 0       \n"," D_88        | 5525447 \n"," B_31        | 0       \n"," S_19        | 0       \n"," R_19        | 0       \n"," B_32        | 0       \n"," S_20        | 0       \n"," R_20        | 75      \n"," R_21        | 0       \n"," B_33        | 2016    \n"," D_89        | 29563   \n"," R_22        | 0       \n"," R_23        | 0       \n"," D_91        | 157216  \n"," D_92        | 0       \n"," D_93        | 0       \n"," D_94        | 0       \n"," R_24        | 0       \n"," R_25        | 0       \n"," D_96        | 0       \n"," S_22        | 19024   \n"," S_23        | 445     \n"," S_24        | 18593   \n"," S_25        | 12847   \n"," S_26        | 634     \n"," D_102       | 40655   \n"," D_103       | 101548  \n"," D_104       | 101548  \n"," D_105       | 3021431 \n"," D_106       | 4990102 \n"," D_107       | 101548  \n"," B_36        | 0       \n"," B_37        | 56      \n"," R_26        | 4922146 \n"," R_27        | 128703  \n"," B_38        | 2016    \n"," D_108       | 5502513 \n"," D_109       | 1597    \n"," D_110       | 5500117 \n"," D_111       | 5500117 \n"," B_39        | 5497819 \n"," D_112       | 2650    \n"," B_40        | 53      \n"," S_27        | 1400935 \n"," D_113       | 176716  \n"," D_114       | 176716  \n"," D_115       | 176716  \n"," D_116       | 176716  \n"," D_117       | 176716  \n"," D_118       | 176716  \n"," D_119       | 176716  \n"," D_120       | 176716  \n"," D_121       | 176716  \n"," D_122       | 176716  \n"," D_123       | 176716  \n"," D_124       | 176716  \n"," D_125       | 176716  \n"," D_126       | 116816  \n"," D_127       | 0       \n"," D_128       | 101548  \n"," D_129       | 101548  \n"," B_41        | 690     \n"," B_42        | 5459973 \n"," D_130       | 101548  \n"," D_131       | 101548  \n"," D_132       | 4988874 \n"," D_133       | 42716   \n"," R_28        | 0       \n"," D_134       | 5336752 \n"," D_135       | 5336752 \n"," D_136       | 5336752 \n"," D_137       | 5336752 \n"," D_138       | 5336752 \n"," D_139       | 101548  \n"," D_140       | 40632   \n"," D_141       | 101548  \n"," D_142       | 4587043 \n"," D_143       | 101548  \n"," D_144       | 40727   \n"," D_145       | 101548  \n","\n"]}],"source":["from pyspark.sql.functions import col,isnan, when, count\n","\n","train_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df.columns]).show(vertical=True)"]},{"cell_type":"code","execution_count":16,"id":"2286bbd2","metadata":{},"outputs":[],"source":["#dropping columns with high percentage of null values.\n","columns_to_drop = ['S_2','D_66','D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142']\n","train_df = train_df.drop(*columns_to_drop)"]},{"cell_type":"markdown","id":"a888be8f","metadata":{},"source":["Imputing null values in the column of our data"]},{"cell_type":"code","execution_count":17,"id":"c58f1eea","metadata":{},"outputs":[],"source":["selected_col = np.array(['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145'])"]},{"cell_type":"code","execution_count":18,"id":"b05b8477","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.ml.feature import Imputer\n","\n","imputer = Imputer(\n","    inputCols= selected_col, \n","    outputCols= selected_col\n","    ).setStrategy(\"median\")\n","\n","# Add imputation cols to df\n","train_df = imputer.fit(train_df).transform(train_df)"]},{"cell_type":"code","execution_count":19,"id":"a02a06fd","metadata":{},"outputs":[],"source":["train_df = (train_df.fillna(\"null\", subset=cat_cols))"]},{"cell_type":"markdown","id":"b9435319","metadata":{},"source":["**Recheck the null value count to verify. Drop columns if any new null columns found.**"]},{"cell_type":"code","execution_count":20,"id":"31f5cb56","metadata":{},"outputs":[],"source":["# Dict_Null_recheck = {col:train_df.filter(train_df[col].isNull()).count() for col in train_df.columns}\n","# Dict_Null_recheck"]},{"cell_type":"code","execution_count":21,"id":"4e4ff0e8","metadata":{},"outputs":[],"source":["# from pyspark.sql.functions import col,isnan, when, count\n","\n","# train_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df.columns]).show(vertical=True)"]},{"cell_type":"code","execution_count":22,"id":"2290efa4","metadata":{},"outputs":[],"source":["# columns_to_dropagain = ['D_64','D_68','B_30','B_38','D_114','D_116','D_117','D_120', 'D_126',]\n","# train_df = train_df.drop(*columns_to_dropagain)"]},{"cell_type":"code","execution_count":23,"id":"224abe0e","metadata":{},"outputs":[],"source":["# #final check for null values.\n","# from pyspark.sql.functions import col,isnan, when, count\n","\n","# train_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df.columns]).show(vertical=True)"]},{"cell_type":"code","execution_count":24,"id":"2c360c2d","metadata":{},"outputs":[],"source":["# from pyspark.sql.functions import *\n","# amount_missing_df_again = train_df.select([(count(when(isnan(c) | col(c).isNull(), c))/count(lit(1))).alias(c) for c in train_df.columns])\n","# amount_missing_df_again.show(vertical =True)"]},{"cell_type":"markdown","id":"4ee0cc47","metadata":{},"source":["**String indexer**"]},{"cell_type":"code","execution_count":25,"id":"5668e93c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Create columns aliases\n","cat_index_cols = add_suffix(cat_cols, \"_index\")\n","\n","# Fit StringIndexer\n","indexers = StringIndexer(inputCols=cat_cols, outputCols=cat_index_cols)\n","indexers_model = indexers.fit(train_df)\n","\n","# Transform to data\n","train_df_indexed = indexers_model.transform(train_df)"]},{"cell_type":"code","execution_count":26,"id":"240de5fc","metadata":{},"outputs":[{"data":{"text/plain":["['B_30',\n"," 'B_38',\n"," 'D_114',\n"," 'D_116',\n"," 'D_117',\n"," 'D_120',\n"," 'D_126',\n"," 'D_63',\n"," 'D_64',\n"," 'D_68']"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# See what columns the indexer handle\n","indexers.getInputCols()"]},{"cell_type":"markdown","id":"93ed4f06","metadata":{},"source":["**Onehot encoding**"]},{"cell_type":"code","execution_count":27,"id":"41d84667","metadata":{},"outputs":[],"source":["# Create columns aliases\n","cat_ohe_cols = add_suffix(cat_cols, \"_ohe\")\n","\n","# Fit OneHotEncoder\n","ohe = OneHotEncoder(inputCols=cat_index_cols, outputCols=cat_ohe_cols)\n","ohe_model = ohe.fit(train_df_indexed)\n","\n","# Transform to data\n","train_df_ohed = ohe_model.transform(train_df_indexed)"]},{"cell_type":"markdown","id":"1766c3c5","metadata":{},"source":["**Data Aggregation**"]},{"cell_type":"code","execution_count":28,"id":"a78453ea","metadata":{},"outputs":[],"source":["# Functions for each type\n","# each tuple consist of: (function, column's suffix)\n","num_funcs = [\n","    (F.mean, \"_mean\"),\n","#     (F.stddev, \"_std\"),\n","    (F.min, \"_min\"),\n","    (F.max, \"_max\"),\n","]\n","\n","cat_funcs = [\n","    (F.count, \"_count\"),\n","    (F.last, \"_last\"),\n","    (F.countDistinct, \"_nunique\"),\n","]"]},{"cell_type":"code","execution_count":29,"id":"f4ec3f76","metadata":{},"outputs":[{"data":{"text/plain":["Column<'avg(P_2) AS `P_2_mean`'>"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# Arguments for .agg method\n","# each arg consist of: func(colname).alias(colname + suffix)\n","import itertools \n","agg_num_args = [\n","    func(col).alias(col + suffix) \n","    for col, (func, suffix) in itertools.product(selected_col, num_funcs)]\n","\n","agg_cols_args = [\n","    func(col).alias(col + suffix) \n","    for col, (func, suffix) in itertools.product(cat_ohe_cols, cat_funcs)]\n","\n","# Combine numeric and categoric agg arguments\n","agg_args = agg_num_args + agg_cols_args\n","agg_args[0]"]},{"cell_type":"code","execution_count":30,"id":"395af397","metadata":{},"outputs":[{"data":{"text/plain":["273"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["len(agg_num_args)"]},{"cell_type":"code","execution_count":31,"id":"2b6e0d6b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unused columns 208\n"]}],"source":["# Columns that we won't use\n","unused_cols = cat_cols + num_cols + cat_index_cols + cat_ohe_cols\n","print(f\"Unused columns {len(unused_cols)}\")"]},{"cell_type":"code","execution_count":32,"id":"20ac8b8b","metadata":{},"outputs":[],"source":["# Apply the agg while also dropping unused columns\n","train_df_grouped = (train_df_ohed.groupBy(\"customer_ID\")\n","                                 .agg(*agg_args)\n","                                 .drop(*unused_cols))"]},{"cell_type":"code","execution_count":33,"id":"7e432947","metadata":{},"outputs":[],"source":["# #final check for null values.\n","# from pyspark.sql.functions import col,isnan, when, count\n","\n","# train_df_grouped.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df_grouped.columns]).show(vertical=True)"]},{"cell_type":"code","execution_count":34,"id":"52029af5","metadata":{},"outputs":[],"source":["# train_df_grouped"]},{"cell_type":"code","execution_count":35,"id":"73a7e979","metadata":{},"outputs":[],"source":["#joining they labels with training data\n","\n","train_joined_df = train_df_grouped.join(F.broadcast(label_df), on=\"customer_ID\")"]},{"cell_type":"code","execution_count":36,"id":"66fa7d06","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total features: 305\n"]}],"source":["#Total number of features\n","dim = len(train_joined_df.columns)\n","print(f\"Total features: {dim}\")"]},{"cell_type":"code","execution_count":37,"id":"b84bca99","metadata":{},"outputs":[],"source":["va = VectorAssembler(\n","    inputCols=train_joined_df.drop(\"customer_ID\", \"target\").columns,\n","    outputCol=\"features\",\n","    handleInvalid=\"error\",\n",")"]},{"cell_type":"code","execution_count":38,"id":"d16fce3b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["train_ready_df = (va.transform(train_joined_df)\n","                    .select([\"customer_ID\", \"features\", \"target\"])\n","                    .persist(StorageLevel.DISK_ONLY))"]},{"cell_type":"code","execution_count":39,"id":"96db191a","metadata":{},"outputs":[{"data":{"text/plain":["[('customer_ID', 'string'), ('features', 'vector'), ('target', 'float')]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["train_ready_df.dtypes"]},{"cell_type":"markdown","id":"c77006b0","metadata":{},"source":["**ML models: Logistic regression, Decision tree, Random forest**"]},{"cell_type":"markdown","id":"042d968b","metadata":{},"source":["Split training and test data using the training data only."]},{"cell_type":"code","execution_count":40,"id":"13e4c686","metadata":{},"outputs":[],"source":["train_split, test_split = train_ready_df.randomSplit(weights = [0.80, 0.20], seed = 100)"]},{"cell_type":"markdown","id":"12297640","metadata":{},"source":["**Logistic Regression**"]},{"cell_type":"code","execution_count":41,"id":"cd092ede","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/14 01:55:24 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","22/12/14 01:55:24 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","                                                                                \r"]}],"source":["logres = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\n","logres_model = logres.fit(train_split)"]},{"cell_type":"code","execution_count":42,"id":"03b636a2","metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[customer_ID: string, features: vector, target: float, rawPrediction: vector, probability: vector, prediction: double]"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["test_pred_lr = logres_model.transform(test_split)\n","test_pred_lr"]},{"cell_type":"markdown","id":"0e1c05d6","metadata":{},"source":["**Decision Tree**"]},{"cell_type":"code","execution_count":46,"id":"4d0e335c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.ml.classification import DecisionTreeClassifier\n","dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"target\")\n","dt_model = dt.fit(train_split)"]},{"cell_type":"code","execution_count":47,"id":"65ff51c9","metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[customer_ID: string, features: vector, target: float, rawPrediction: vector, probability: vector, prediction: double]"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["test_pred_dt = dt_model.transform(test_split)\n","test_pred_dt"]},{"cell_type":"markdown","id":"5e08c390","metadata":{},"source":["**Random Forest**"]},{"cell_type":"code","execution_count":48,"id":"d7086095","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/13 07:00:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1317.1 KiB\n","22/12/13 07:01:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2009.3 KiB\n","22/12/13 07:01:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n","                                                                                \r"]}],"source":["from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","#Training Model\n","rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'target', numTrees = 500, maxDepth = 5)\n","rfModel = rf.fit(train_split)"]},{"cell_type":"code","execution_count":49,"id":"00ad6eda","metadata":{},"outputs":[],"source":["#Prediction\n","test_pred_rf = rfModel.transform(test_split)"]},{"cell_type":"code","execution_count":50,"id":"882ca8ca","metadata":{},"outputs":[],"source":["ml = {}"]},{"cell_type":"code","execution_count":51,"id":"cd47aa5d","metadata":{},"outputs":[],"source":["ml['test_pred_lr'] = test_pred_lr\n","ml['test_pred_dt'] = test_pred_dt\n","ml['test_pred_rf'] = test_pred_rf"]},{"cell_type":"code","execution_count":52,"id":"c487aec7","metadata":{},"outputs":[{"data":{"text/plain":["{'test_pred_lr': DataFrame[customer_ID: string, features: vector, target: float, rawPrediction: vector, probability: vector, prediction: double],\n"," 'test_pred_dt': DataFrame[customer_ID: string, features: vector, target: float, rawPrediction: vector, probability: vector, prediction: double],\n"," 'test_pred_rf': DataFrame[customer_ID: string, features: vector, target: float, rawPrediction: vector, probability: vector, prediction: double]}"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["ml"]},{"cell_type":"markdown","id":"b34b5110","metadata":{},"source":["**Evaluations**"]},{"cell_type":"markdown","id":"848745a4","metadata":{},"source":["**Evaluating using Pyspark metric**"]},{"cell_type":"code","execution_count":53,"id":"b4f9662d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["accuracy_test_pred_lr 0.8876329787234043\n","f1_test_pred_lr 0.8867674993644983\n","weightedPrecision_test_pred_lr 0.8861970497152565\n","weightedRecall_test_pred_lr 0.8876329787234043\n","auc_test_pred_lr 0.8451827179103529\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["accuracy_test_pred_dt 0.8595221485873735\n","f1_test_pred_dt 0.8588990929851348\n","weightedPrecision_test_pred_dt 0.858370318265501\n","weightedRecall_test_pred_dt 0.8595221485873735\n","auc_test_pred_dt 0.8113804314901693\n"]},{"name":"stderr","output_type":"stream","text":["22/12/13 07:03:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n","22/12/13 07:03:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n","22/12/13 07:03:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n","22/12/13 07:03:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n","22/12/13 07:03:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n","[Stage 669:====================================================>(197 + 3) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["accuracy_test_pred_rf 0.8666070805720265\n","f1_test_pred_rf 0.8648602210811689\n","weightedPrecision_test_pred_rf 0.8639013640002504\n","weightedRecall_test_pred_rf 0.8666070805720265\n","auc_test_pred_rf 0.8127272435136679\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","\n","# Create both evaluators\n","evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\")\n","evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n","\n","# Make predicitons\n","# predictionAndTarget = model.transform(df).select(\"target\", \"prediction\")\n","for key, value in ml.items(): \n","# Get metrics\n","#      acc = \"acc_{}\".format(i)\n","#      f1 = \"f1_{}\".format(i)\n","#      weightedPrecision = \"weightedPrecision_{}\".format(i)\n","#      weightedRecall = \"weightedRecall_{}\".format(i)\n","#      auc = \"auc_{}\".format(i)   \n","#      test_pred = \"test_pred_{}\".format(i)\n","     acc = evaluatorMulti.evaluate(value, {evaluatorMulti.metricName: \"accuracy\"})\n","     f1 = evaluatorMulti.evaluate(value, {evaluatorMulti.metricName: \"f1\"})\n","     weightedPrecision = evaluatorMulti.evaluate(value, {evaluatorMulti.metricName: \"weightedPrecision\"})\n","     weightedRecall = evaluatorMulti.evaluate(value, {evaluatorMulti.metricName: \"weightedRecall\"})\n","     auc = evaluator.evaluate(value)\n","#      prnt(test_pred)\n","     print(\"accuracy_{}\".format(key), acc)\n","     print(\"f1_{}\".format(key), f1)\n","     print(\"weightedPrecision_{}\".format(key), weightedPrecision)\n","     print(\"weightedRecall_{}\".format(key),weightedRecall)\n","     print(\"auc_{}\".format(key),auc)"]},{"cell_type":"markdown","id":"5a059c02","metadata":{},"source":["**Evaluation using AMEX metric**"]},{"cell_type":"code","execution_count":54,"id":"c341b1a5","metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n","\n","    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n","        df = (pd.concat([y_true, y_pred], axis='columns')\n","              .sort_values('prediction', ascending=False))\n","        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n","        four_pct_cutoff = int(0.04 * df['weight'].sum())\n","        df['weight_cumsum'] = df['weight'].cumsum()\n","        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n","        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n","        \n","    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n","        df = (pd.concat([y_true, y_pred], axis='columns')\n","              .sort_values('prediction', ascending=False))\n","        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n","        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n","        total_pos = (df['target'] * df['weight']).sum()\n","        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n","        df['lorentz'] = df['cum_pos_found'] / total_pos\n","        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n","        return df['gini'].sum()\n","\n","    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n","        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n","        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n","\n","    g = normalized_weighted_gini(y_true, y_pred)\n","    d = top_four_percent_captured(y_true, y_pred)\n","\n","    return 0.5 * (g + d)"]},{"cell_type":"code","execution_count":55,"id":"10bc6df4","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["amex_accuracy_test_pred_lr 0.5395285451031633\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["amex_accuracy_test_pred_dt 0.45006891837885576\n"]},{"name":"stderr","output_type":"stream","text":["22/12/13 07:03:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["amex_accuracy_test_pred_rf 0.47386258130733566\n"]}],"source":["for key, value in ml.items():\n","    y_pred = pd.DataFrame((value.select([\"customer_ID\", \"prediction\"])\n","                                  .toPandas()))\n","\n","    y_true = pd.DataFrame((value.select([\"customer_ID\", \"target\"])\n","                                  .toPandas()))\n","\n","    amex_accuracy = amex_metric(y_true, y_pred)\n","    print(\"amex_accuracy_{}\".format(key),amex_accuracy)\n","                          "]},{"cell_type":"markdown","id":"7ef8d0b9","metadata":{},"source":["**Save model**"]},{"cell_type":"code","execution_count":56,"id":"1b311446","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["logres_model.save(\"gs://amex-data/saved_models/lr_model\")"]},{"cell_type":"markdown","id":"ccc325f0","metadata":{},"source":["**Explainable model using game theory like Shapley**"]},{"cell_type":"code","execution_count":46,"id":"36952d04","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: shap in /opt/conda/miniconda3/lib/python3.8/site-packages (0.41.0)\n","Requirement already satisfied: scipy in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (1.6.3)\n","Requirement already satisfied: slicer==0.0.7 in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (0.0.7)\n","Requirement already satisfied: cloudpickle in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (2.2.0)\n","Requirement already satisfied: tqdm>4.25.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (4.64.1)\n","Requirement already satisfied: numba in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (0.53.1)\n","Requirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (1.2.5)\n","Requirement already satisfied: packaging>20.9 in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (21.3)\n","Requirement already satisfied: scikit-learn in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (0.24.2)\n","Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from shap) (1.19.5)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from packaging>20.9->shap) (2.4.7)\n","Requirement already satisfied: setuptools in /opt/conda/miniconda3/lib/python3.8/site-packages (from numba->shap) (59.8.0)\n","Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from numba->shap) (0.36.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->shap) (2.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->shap) (2022.6)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from scikit-learn->shap) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /opt/conda/miniconda3/lib/python3.8/site-packages (from scikit-learn->shap) (1.2.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.16.0)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install shap"]},{"cell_type":"code","execution_count":null,"id":"97b26387","metadata":{},"outputs":[],"source":["ct.fit(X)\n","X_shap = ct.fit_transform(X)\n","test_shap  = ct.transform(test)\n","explainer = shap.LinearExplainer(logr_pipe.named_steps['LR'], X_shap, feature_perturbation=\"interventional\")\n","shap_values = explainer.shap_values(test_shap)"]},{"cell_type":"code","execution_count":null,"id":"8e1de464","metadata":{},"outputs":[],"source":["# Evaluate SHAP values\n","shap_values = explainer.shap_values(X)"]},{"cell_type":"code","execution_count":null,"id":"83325877","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}