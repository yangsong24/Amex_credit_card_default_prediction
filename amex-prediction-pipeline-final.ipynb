{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51280c65",
   "metadata": {},
   "source": [
    "# Credit Default Prediction on Amex Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae33d16",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab2bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import pyspark\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import (\n",
    "    SparkSession, \n",
    "    types, \n",
    "    functions as F,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    isnan,\n",
    "    when,\n",
    "    count,\n",
    ")\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder, \n",
    "    StringIndexer, \n",
    "    VectorAssembler, \n",
    "    Imputer,\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, \n",
    "    LinearSVC,\n",
    "    DecisionTreeClassifier,\n",
    "    GBTClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    MulticlassClassificationEvaluator,\n",
    ")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17bb5c5",
   "metadata": {},
   "source": [
    "### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccb9862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/16 02:38:54 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/12/16 02:38:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/12/16 02:38:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/12/16 02:38:54 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"amex-app\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11645b2",
   "metadata": {},
   "source": [
    "### Important Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd5eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'gs://icdp-bigdata-bucket/train_data.csv'\n",
    "TRAIN_LABEL_PATH = 'gs://icdp-bigdata-bucket/train_labels.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021fff4c",
   "metadata": {},
   "source": [
    "### Miscellaneous Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838521ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a Schema Object for the Dataframe \n",
    "def create_spark_schema(series):\n",
    "    fields = list()\n",
    "    \n",
    "    for value in series: \n",
    "        if value in string_dtypes:\n",
    "            fields.append(\n",
    "                types.StructField(\n",
    "                    value, \n",
    "                    types.StringType(), \n",
    "                    True,\n",
    "                )\n",
    "            )\n",
    "        elif value in date_dtypes:\n",
    "            fields.append(\n",
    "                types.StructField(\n",
    "                    value, \n",
    "                    types.DateType(), \n",
    "                    True,\n",
    "                )\n",
    "            )\n",
    "        elif value in integer_dtypes:\n",
    "            fields.append(\n",
    "                types.StructField(\n",
    "                    value, \n",
    "                    types.IntegerType(), \n",
    "                    True,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            fields.append(\n",
    "                types.StructField(\n",
    "                    value, \n",
    "                    types.FloatType(), \n",
    "                    True,\n",
    "                )\n",
    "            )\n",
    "    return types.StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25fa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Suffix to List Elements\n",
    "def add_suffix(names, suffix):\n",
    "    return [name + suffix for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac25dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns with Null values above a certain threshold\n",
    "def dropNullColumns(df, threshold):\n",
    "    \"\"\"\n",
    "    This function drops columns containing all null values.\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "  \n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(\n",
    "        c) for c in df.columns]).collect()[0].asDict()\n",
    "    print(\"null counts calculated...\")\n",
    "    df_count = df.count()\n",
    "    col_to_drop = [k for k, v in null_counts.items() if v >(df_count * threshold)]  \n",
    "    print(\"columns to drop found...\")\n",
    "    df = df.drop(*col_to_drop)  \n",
    "  \n",
    "    return df, col_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14342a4",
   "metadata": {},
   "source": [
    "### Reading the Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd033b",
   "metadata": {},
   "source": [
    "#### Reading the First 20 rows only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095cf615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df_temp = spark.read.option(\n",
    "    \"header\", 'true',\n",
    ").csv(\n",
    "    TRAIN_DATA_PATH,\n",
    ").limit(\n",
    "    20\n",
    ")\n",
    "train_labels_temp = spark.read.option(\n",
    "    \"header\", 'true',\n",
    ").csv(\n",
    "    TRAIN_LABEL_PATH,\n",
    ").limit(\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e07dfe",
   "metadata": {},
   "source": [
    "#### Define Schema Using Sampled Temporary Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc3d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Known Datatypes: \n",
    "\n",
    "string_dtypes = [\"customer_ID\", 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "date_dtypes = ['S_2']\n",
    "integer_dtypes = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a141c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schema = create_spark_schema(train_df_temp.columns)\n",
    "label_schema = create_spark_schema(train_labels_temp.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a1241",
   "metadata": {},
   "source": [
    "#### Remove Temp Datasets from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72256a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_temp.unpersist()\n",
    "train_labels_temp.unpersist()\n",
    "\n",
    "del train_df_temp\n",
    "del train_labels_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda7552",
   "metadata": {},
   "source": [
    "#### Reading the Whole Dataset with the Inferred Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a20d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.option(\n",
    "    \"header\", \n",
    "    \"true\",\n",
    ").csv(\n",
    "    TRAIN_DATA_PATH, \n",
    "    schema=train_schema\n",
    ")\n",
    "label_df = spark.read.option(\n",
    "    \"header\", \n",
    "    \"true\",\n",
    ").csv(\n",
    "    TRAIN_LABEL_PATH, \n",
    "    schema=label_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667c95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other categorization of the known dtypes\n",
    "info_cols = ['customer_ID', 'S_2']\n",
    "target_cols = ['target']\n",
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "\n",
    "# Define Numeric Columns\n",
    "excluded = info_cols + cat_cols\n",
    "num_cols = [col for col in train_df.columns if col not in excluded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057682b",
   "metadata": {},
   "source": [
    "### Preprocessing of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d994039",
   "metadata": {},
   "source": [
    "#### Dropping Null Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d8727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/16 02:39:08 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null counts calculated...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================> (120 + 3) / 123]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns to drop found...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Remove All Columns with More than 5% Missing Values\n",
    "train_df, cols_to_drop = dropNullColumns(train_df, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b378259",
   "metadata": {},
   "source": [
    "#### Remove Less Important Column S_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ff3fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the S_2 variable as the testing data and the training data are in different time periods \n",
    "train_df = train_df.drop(\"S_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9421e4",
   "metadata": {},
   "source": [
    "#### Converting Categorical Columns to Numeric using StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce262b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns_to_index = list(set(train_df.columns) & set(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c771e018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cat_cols_indexed = add_suffix(cat_columns_to_index, \"_index\")\n",
    "\n",
    "## Create StringIndexer Object\n",
    "indexer = StringIndexer(\n",
    "    inputCols=cat_columns_to_index,\n",
    "    outputCols=cat_cols_indexed,\n",
    ")\n",
    "indexer.setHandleInvalid(\"keep\")\n",
    "indexer_model = indexer.fit(train_df)\n",
    "\n",
    "train_df = indexer_model.transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4e689",
   "metadata": {},
   "source": [
    "#### Impute values for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99a2838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns_to_impute = list(set(train_df.columns) & set(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f36507d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_cols_imputed = add_suffix(num_columns_to_impute, \"_imputed\")\n",
    "\n",
    "##Create Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=num_columns_to_impute,\n",
    "    outputCols=num_cols_imputed,\n",
    ")\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "imputer_model = imputer.fit(train_df)\n",
    "\n",
    "train_df = imputer_model.transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd655207",
   "metadata": {},
   "source": [
    "#### OneHotEncode the Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09fedfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_ohe = add_suffix(cat_cols_indexed, \"_ohe\")\n",
    "https://github.com/yangsong24/Amex_credit_card_default_prediction.git\n",
    "### Create Ohe Object\n",
    "ohe = OneHotEncoder(\n",
    "    inputCols = cat_cols_indexed,\n",
    "    outputCols = cat_cols_ohe,\n",
    ")\n",
    "\n",
    "ohe_model = ohe.fit(train_df)\n",
    "\n",
    "train_df = ohe_model.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abf2043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_cols = [\"customer_ID\"] + cat_cols_ohe + num_cols_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a034df5",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Columns and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebf5573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.select(*useful_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c61731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_num_cols = []\n",
    "for num_col in num_cols_imputed:\n",
    "    new_name = num_col.split(\"_\")[0] + \"_\" + num_col.split(\"_\")[1]\n",
    "    new_num_cols.append(new_name)\n",
    "    train_df = train_df.withColumnRenamed(num_col, new_name)\n",
    "new_cat_cols = []\n",
    "for cat_col in cat_cols_ohe:\n",
    "    new_name = cat_col.split(\"_\")[0] + \"_\" + cat_col.split(\"_\")[1]\n",
    "    new_cat_cols.append(new_name)\n",
    "    train_df = train_df.withColumnRenamed(cat_col, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "401bdaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregation Functions\n",
    "num_funcs = [\n",
    "    (F.mean, \"_mean\"),\n",
    "     (F.min, \"_min\"),\n",
    "     (F.max, \"_max\"),\n",
    "]\n",
    "\n",
    "cat_funcs = [\n",
    "    (F.count, \"_count\"),\n",
    "    (F.last, \"_last\"),\n",
    "    (F.countDistinct, \"_nunique\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "839fe2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_num_args = [\n",
    "    func(col).alias(col + suffix) \n",
    "    for col, (func, suffix) in itertools.product(new_num_cols, num_funcs)]\n",
    "\n",
    "agg_cols_args = [\n",
    "    func(col).alias(col + suffix) \n",
    "    for col, (func, suffix) in itertools.product(new_cat_cols, cat_funcs)]\n",
    "\n",
    "# Combine numeric and categoric agg arguments\n",
    "agg_args = agg_num_args + agg_cols_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18cd5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.groupBy(\"customer_ID\").agg(*agg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7b0f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(\n",
    "    F.broadcast(label_df), \n",
    "    on=\"customer_ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b25a0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_model = VectorAssembler(\n",
    "    inputCols=train_df.drop(\n",
    "        \"customer_ID\",\n",
    "        \"target\",\n",
    "    ).columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87c6b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df = va_model.transform(\n",
    "    train_df,\n",
    ").select(\n",
    "    [\n",
    "        \"customer_ID\", \n",
    "        \"features\", \n",
    "        \"target\",\n",
    "    ]\n",
    ").persist(\n",
    "    StorageLevel.DISK_ONLY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b8328",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfa156aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = train_df.randomSplit(weights = [0.8, 0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757a1d7",
   "metadata": {},
   "source": [
    "### Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57fb58",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc49c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/16 03:49:38 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/16 03:49:38 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"target\",\n",
    ")\n",
    "lr_model = lr.fit(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr_model.transform(test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "binEval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\",metricName=\"areaUnderROC\")\n",
    "multiEval = MulticlassClassificationEvaluator(labelCol = \"target\", predictionCol = \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c9a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCROC:  0.8495909980592996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8898278218092375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.8889484226843926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision:  0.8884047070122425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 585:===================================================> (194 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Recall:  0.8898278218092375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"AUCROC: \", binEval.evaluate(lr_preds))\n",
    "print(\"Accuracy: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"accuracy\"}))\n",
    "print(\"F1 Score: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"f1\"}))\n",
    "print(\"Weighted Precision: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"weightedPrecision\"}))\n",
    "print(\"Weighted Recall: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"weightedRecall\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
